{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4 - Linear versus Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend Bob just moved to Boston. He is a real estate agent who is trying to evaluate the prices of houses in the Boston area. He has been using a linear regression model but he wonders if he can improve his accuracy on predicting the prices for new houses. He comes to you for help as he knows that you're an expert in machine learning. \n",
    "\n",
    "As a pro, you suggest doing a *polynomial transformation*  to create a more flexible model, and performing ridge regression since having so many features compared to data points increases the variance. \n",
    "\n",
    "Bob, however, being a skeptic isn't convinced. He wants you to write a program that illustrates the difference in training and test costs for both linear and ridge regression on the same dataset. Being a good friend, you oblige and hence this assignment :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you are to explore the effects of ridge regression.  We will use a dataset that is part of the sklearn.dataset package.  Learn more at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
    "\n",
    "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boston dataset from sklearn\n",
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is:  13\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "The number of exampels in our dataset:  506\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "#  Create X and Y variables - X holding the .data and Y holding .target \n",
    "X = boston_data.data\n",
    "y = boston_data.target\n",
    "\n",
    "#  Reshape Y to be a rank 2 matrix \n",
    "y = y.reshape(X.shape[0], 1)\n",
    "\n",
    "# Observe the number of features and the number of labels\n",
    "print('The number of features is: ', X.shape[1])\n",
    "# Printing out the features\n",
    "print('The features: ', boston_data.feature_names)\n",
    "# The number of examples\n",
    "print('The number of exampels in our dataset: ', X.shape[0])\n",
    "#Observing the first 2 rows of the data\n",
    "print(X[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. \n",
    "# Transform X and save it into X_2. Simply copy Y into Y_2 \n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_2 = poly.fit_transform(X)\n",
    "y_2 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(y_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# TODO - Return w values\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
    "    # use np.linalg.pinv(a)\n",
    "    I = np.identity(X_train.shape[1])\n",
    "    inv = np.dot(X_train.T, X_train) + alpha*I\n",
    "    inv = np.linalg.pinv(inv)\n",
    "    w = np.dot(inv, X_train.T)\n",
    "    w = np.dot(w, y_train)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the evaluate_err_ridge function.\n",
    "# TODO - Return the train_error and test_error values\n",
    "\n",
    "\n",
    "def evaluate_err(X_train, X_test, y_train, y_test, w):\n",
    "    y_hat = np.dot(X_train, w)\n",
    "    ytest_hat = np.dot(X_test, w)\n",
    "    \n",
    "    train_error = np.sum(np.square(y_hat - y_train)) / len(X_train)\n",
    "    test_error = np.sum(np.square(ytest_hat - y_test)) / len(X_test)\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Finish writting the k_fold_cross_validation function. \n",
    "# TODO - Returns the average training error and average test error from the k-fold cross validation\n",
    "# use Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "def k_fold_cross_validation(k, X, y, alpha):\n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_E_val_test = 0\n",
    "    total_E_val_train = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # scaling the data matrix\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Adding a column of 1's to the front of X_train and X_test\n",
    "        one_col = np.ones((X_train.shape[0],1))\n",
    "        X_train = np.hstack((one_col, X_train ))\n",
    "        one_col = np.ones((X_test.shape[0],1))\n",
    "        X_test  = np.hstack((one_col, X_test ))\n",
    "        \n",
    "        # determine the training error and the test error\n",
    "        w = get_coeff_ridge_normaleq(X_train, y_train, alpha)\n",
    "        \n",
    "        train_error, test_error = evaluate_err(X_train, X_test, y_train, y_test, w)\n",
    "        total_E_val_test += test_error\n",
    "        total_E_val_train += train_error\n",
    "        \n",
    "    E_val_test = total_E_val_test / k\n",
    "    E_val_train = total_E_val_train / k\n",
    "    return  E_val_test, E_val_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression model, using 10-fold cross validation.\n",
      "MSE for test:  23.63606860542821\n",
      "MSE for train:  21.806183575851065\n"
     ]
    }
   ],
   "source": [
    "# For 1(a)\n",
    "test_val, train_val = k_fold_cross_validation(10, X, y, 0.0)\n",
    "print(\"Linear regression model, using 10-fold cross validation.\")\n",
    "print(\"MSE for test: \", test_val)\n",
    "print(\"MSE for train: \", train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression model, using 10-fold cross validation.\n",
      "When alpha is 1.023292992280754, train val is 21.810064209445805, test val is 23.635106720354944\n",
      "When alpha is 1.2373711899353526, train val is 21.811833657592125, test val is 23.635883832250133\n",
      "When alpha is 1.4962356560944334, train val is 21.814403182714514, test val is 23.637262110482403\n",
      "When alpha is 1.8092559102538208, train val is 21.81812978704581, test val is 23.639560342109444\n",
      "When alpha is 2.1877616239495525, train val is 21.823526530670666, test val is 23.643246542987903\n",
      "When alpha is 2.6454526947240495, train val is 21.831328668836225, test val is 23.64900296332241\n",
      "When alpha is 3.198895109691398, train val is 21.842586598001407, test val is 23.657817679646804\n",
      "When alpha is 3.868120546330522, train val is 21.85879586553974, test val is 23.671112946059374\n",
      "When alpha is 4.677351412871983, train val is 21.882078198982207, test val is 23.69092414856501\n",
      "When alpha is 5.6558775708915405, train val is 21.91543252678735, test val is 23.720148200409486\n",
      "When alpha is 6.839116472814294, train val is 21.963081784461387, test val is 23.762887047018033\n",
      "When alpha is 8.26989508567932, train val is 22.03095056479587, test val is 23.824921291866\n",
      "When alpha is 10.0, train val is 22.127321102577064, test val is 23.914361582953426\n",
      "Average MSE for train set is:  21.886887129034633\n",
      "Average MSE for test set is:  23.698633492925026\n"
     ]
    }
   ],
   "source": [
    "# For 1(b)\n",
    "alpha = np.logspace(.01, 1, num=13)\n",
    "sum_train, sum_test = 0, 0\n",
    "print(\"Ridge regression model, using 10-fold cross validation.\")\n",
    "for i in alpha:\n",
    "    test_val, train_val = k_fold_cross_validation(10, X, y, i)\n",
    "    sum_train += train_val\n",
    "    sum_test += test_val\n",
    "    print(\"When alpha is {}, train val is {}, test val is {}\".format(i, train_val, test_val))\n",
    "   \n",
    "print(\"Average MSE for train set is: \", sum_train / len(alpha))\n",
    "print(\"Average MSE for test set is: \", sum_test / len(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression model, using 10-fold cross validation.\n",
      "When new alpha is 10.0, train val is 22.127321102577064, test val is 23.914361582953426\n",
      "When new alpha is 21.544346900318832, train val is 23.12442428565689, test val is 24.86756740552452\n",
      "When new alpha is 46.41588833612777, train val is 26.934135882264844, test val is 28.593239993155418\n",
      "When new alpha is 100.0, train val is 40.18568455612646, test val is 41.70924340688511\n",
      "When new alpha is 215.44346900318823, train val is 78.51808672478744, test val is 79.87911878294982\n",
      "When new alpha is 464.15888336127773, train val is 160.252465954425, test val is 161.45030820065497\n",
      "When new alpha is 1000.0, train val is 278.23915817295574, test val is 279.2368421240491\n",
      "When new alpha is 2154.4346900318824, train val is 394.5357675343761, test val is 395.27692442129177\n",
      "When new alpha is 4641.588833612777, train val is 480.5518587815656, test val is 481.0402255237194\n",
      "When new alpha is 10000.0, train val is 533.7363430617409, test val is 534.0364502509802\n",
      "When new alpha is 21544.346900318822, train val is 563.1270511472828, test val is 563.3129916145838\n",
      "When new alpha is 46415.888336127726, train val is 578.1901582042011, test val is 578.3151795352727\n",
      "When new alpha is 100000.0, train val is 585.5544340616286, test val is 585.6491640702567\n",
      "Average MSE for train set with new aplha is:  289.62129918996834\n",
      "Average MSE for test set with new aplha is:  290.56012437786745\n"
     ]
    }
   ],
   "source": [
    "# For 1(c)\n",
    "alpha = np.logspace(1, 5, num=13)\n",
    "sum_train, sum_test = 0, 0\n",
    "print(\"Ridge regression model, using 10-fold cross validation.\")\n",
    "for i in alpha:\n",
    "    test_val, train_val = k_fold_cross_validation(10, X, y, i)\n",
    "    sum_train += train_val\n",
    "    sum_test += test_val\n",
    "    print(\"When new alpha is {}, train val is {}, test val is {}\".format(i, train_val, test_val))\n",
    "   \n",
    "print(\"Average MSE for train set with new aplha is: \", sum_train / len(alpha))\n",
    "print(\"Average MSE for test set with new aplha is: \", sum_test / len(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression model, using 10-fold cross validation, with polynomial transformation.\n",
      "MSE for test:  11.854968234785357\n",
      "MSE for train:  5.8088208160124655\n",
      "Ridge regression model, using 10-fold cross validation, with polynomial transformation.\n",
      "When alpha is 1.023292992280754, train val is 7.4316048993421875, test val is 11.755772615671592\n",
      "When alpha is 1.2373711899353526, train val is 7.578014628504877, test val is 11.83641981464214\n",
      "When alpha is 1.4962356560944334, train val is 7.73444342388571, test val is 11.924715611946223\n",
      "When alpha is 1.8092559102538208, train val is 7.9018145020924475, test val is 12.02130157712696\n",
      "When alpha is 2.1877616239495525, train val is 8.081235046549736, test val is 12.127135874177302\n",
      "When alpha is 2.6454526947240495, train val is 8.274115904325258, test val is 12.24368328970293\n",
      "When alpha is 3.198895109691398, train val is 8.482345997128379, test val is 12.373154458018675\n",
      "When alpha is 3.868120546330522, train val is 8.708517479370503, test val is 12.518770613700134\n",
      "When alpha is 4.677351412871983, train val is 8.956190177237655, test val is 12.685025845156018\n",
      "When alpha is 5.6558775708915405, train val is 9.230179979615087, test val is 12.877922750698053\n",
      "When alpha is 6.839116472814294, train val is 9.536858986618622, test val is 13.105170157796167\n",
      "When alpha is 8.26989508567932, train val is 9.884468842681887, test val is 13.376354439556504\n",
      "When alpha is 10.0, train val is 10.283475819125439, test val is 13.703131052044833\n",
      "Average MSE for train set is:  8.6217896681906\n",
      "Average MSE for test set is:  12.503735238479807\n"
     ]
    }
   ],
   "source": [
    "# For 2\n",
    "test_val, train_val = k_fold_cross_validation(10, X_2, y_2, 0.0)\n",
    "print(\"Linear regression model, using 10-fold cross validation, with polynomial transformation.\")\n",
    "print(\"MSE for test: \", test_val)\n",
    "print(\"MSE for train: \", train_val)\n",
    "\n",
    "alpha = np.logspace(.01, 1, num=13)\n",
    "sum_train, sum_test = 0, 0\n",
    "print(\"Ridge regression model, using 10-fold cross validation, with polynomial transformation.\")\n",
    "for i in alpha:\n",
    "    test_val, train_val = k_fold_cross_validation(10, X_2, y_2, i)\n",
    "    sum_train += train_val\n",
    "    sum_test += test_val\n",
    "    print(\"When alpha is {}, train val is {}, test val is {}\".format(i, train_val, test_val))\n",
    "   \n",
    "print(\"Average MSE for train set is: \", sum_train / len(alpha))\n",
    "print(\"Average MSE for test set is: \", sum_test / len(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253.1560947929154\n"
     ]
    }
   ],
   "source": [
    "# For 3(b)\n",
    "x = np.array([5, 0.5, 2, 0, 4, 8, 4, 6, 2, 2, 2, 4, 5.5])\n",
    "x_predict = x.reshape(1, 13)\n",
    "x2_predict = poly.fit_transform(x_predict)\n",
    "\n",
    "train_x = X_2\n",
    "train_y = y_2\n",
    "\n",
    "train_y_mean = np.mean(train_y)\n",
    "train_y = train_y - train_y_mean\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "train_x = scaler.transform(train_x)\n",
    "x2_predict = scaler.transform(x2_predict)\n",
    "\n",
    "w = get_coeff_ridge_normaleq(train_x, train_y, 0)\n",
    "prediction = np.dot(x2_predict, w) + train_y_mean\n",
    "print(prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
